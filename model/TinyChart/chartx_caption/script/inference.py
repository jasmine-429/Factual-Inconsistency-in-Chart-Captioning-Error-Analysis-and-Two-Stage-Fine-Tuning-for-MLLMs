import sys
sys.path.append("/data/jguo376/project/model/TinyChart")

import torch
import json
import os
import pandas as pd
import numpy as np
from PIL import Image
from tqdm import tqdm
from scipy.stats import kendalltau
from tinychart.model.builder import load_pretrained_model
from tinychart.mm_utils import get_model_name_from_path
from tinychart.eval.run_tiny_chart import inference_model
from peft import PeftModel
# ========= ç¯å¢ƒé…ç½® =========
os.environ["CUDA_VISIBLE_DEVICES"] = "7"
model_path = "/data/jguo376/pretrained_models/TinyChart-3B-768"
use_lora = True  # ä¿®æ”¹ä¸º True
lora_path = "/data/jguo376/project/model/TinyChart/checkpoints/chartx_caption/checkpoint-800"

# ========= åŠ è½½æ¨¡å‹ =========
print("Loading model...")
tokenizer, model, image_processor, context_len = load_pretrained_model(
    model_path,
    model_base=None,  # è®¾ä¸º None
    model_name=get_model_name_from_path(model_path),
    device="cuda:0"
)

if use_lora:
    print("Loading LoRA...")
    model = PeftModel.from_pretrained(model, lora_path)
    print("Merging LoRA weights...")
    model = model.merge_and_unload()
    print("LoRA weights loaded and merged.")

# å…³é”®ä¿®å¤ï¼šç»Ÿä¸€æ•°æ®ç±»å‹
model = model.half()  # è€Œä¸æ˜¯ model.float()
print(f"Model loaded on device: {next(model.parameters()).device}")
print(f"Model dtype: {next(model.parameters()).dtype}")


# ========= è·¯å¾„é…ç½® =========
dataset_root = "/data/jguo376/project/dataset/test_dataset/ChartX/test_eva_data/data"
output_root = "/data/jguo376/project/model/TinyChart/chartx_caption"
input_jsonl = os.path.join(dataset_root, "eva_test.json")
output_json = os.path.join(output_root, "ft_output.json")
chart_root = "/data/jguo376/project/dataset/ChartX_dataset/ChartX"
prompt = "Please describe the chart."
# ========= åŠ è½½è¾“å…¥æ•°æ® =========
with open(input_jsonl, "r", encoding="utf-8") as f:
    data_list = json.load(f)

# ========= æ–­ç‚¹ç»­è·‘æ”¯æŒï¼šè®°å½•å·²å¤„ç†å›¾ç‰‡ =========
processed_imgs = set()
if os.path.exists(output_json):
    with open(output_json, "r", encoding="utf-8") as f:
        try:
            existing_results = json.load(f)
            for entry in existing_results:
                processed_imgs.add(entry["img"])  # ä½¿ç”¨ç»å¯¹è·¯å¾„ä½œä¸ºå”¯ä¸€æ ‡è¯†
        except Exception:
            existing_results = []
else:
    existing_results = []

results = existing_results.copy()

# ========= å‚æ•°é…ç½® =========
max_test = None      # è‹¥åªè·‘éƒ¨åˆ†ï¼Œå¯è®¾ç½®ä¸ºæ•°å­—ï¼Œå¦‚ 10ï¼›å¦åˆ™è®¾ä¸º None
save_every = 20      # æ¯å¤„ç† N å¼ å›¾ç‰‡ä¿å­˜ä¸€æ¬¡ç»“æœ
count = 0

# ========= å¼€å§‹å¤„ç† =========
print("ğŸš€ Start TinyChart caption generation...")
for item in tqdm(data_list, desc="Generating captions"):
    rel_path = item["img"]
    image_path = os.path.join(chart_root, rel_path.replace("./", ""))

    if image_path in processed_imgs:
        continue

    if max_test is not None and count >= max_test:
        break

    if not os.path.exists(image_path):
        caption = f"[ERROR] Image not found: {image_path}"
    else:
        try:
            caption = inference_model([image_path], prompt, model, tokenizer, image_processor, context_len, conv_mode="phi", max_new_tokens=512)
        except Exception as e:
            caption = f"[ERROR] {str(e)}"

    item["model_name"] = model_path
    item["img"] = image_path  # ç»å¯¹è·¯å¾„
    item["generated_caption"] = caption
    results.append(item)
    processed_imgs.add(image_path)
    count += 1

    print(f"[âœ“] {item.get('imgname')}")
    print(f"    â†’ {caption}\n")

    # ========= ä¸­é—´ä¿å­˜ =========
    if count % save_every == 0:
        with open(output_json, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

# ========= æœ€ç»ˆä¿å­˜ =========
with open(output_json, "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"\nâœ… TinyChart æ¨ç†å®Œæˆï¼å…±å¤„ç† {len(results)} å¼ å›¾ï¼Œè¾“å‡ºä¿å­˜è‡³: {output_json}")
