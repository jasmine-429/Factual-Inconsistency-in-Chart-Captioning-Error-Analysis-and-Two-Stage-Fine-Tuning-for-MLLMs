import sys
sys.path.append("/data/jguo376/project/model/TinyChart")

import os
import torch
from tinychart.model.builder import load_pretrained_model

os.environ["CUDA_VISIBLE_DEVICES"] = "6"

# è·¯å¾„é…ç½®
base_model_path = "/data/jguo376/pretrained_models/TinyChart-3B-768"
lora_path = "/data/jguo376/project/model/TinyChart/checkpoints/chart_entail/checkpoint-800"

def load_lora_model():
    print("=== åŠ è½½ TinyChart LoRA æ¨¡å‹ ===")
    
    try:
        # åŠ è½½åŸºç¡€æ¨¡å‹
        tokenizer, model, image_processor, context_len = load_pretrained_model(
            model_path=base_model_path,
            model_base=None,
            model_name="tinychart",
            device="cuda"
        )
        
        # æ‰‹åŠ¨åŠ è½½ LoRA
        from peft import PeftModel
        model = PeftModel.from_pretrained(model, lora_path)
        model = model.merge_and_unload()
        
        # å…³é”®ä¿®å¤ï¼šç»Ÿä¸€æ•°æ®ç±»å‹
        model = model.half()
        
        print("âœ… LoRA åŠ è½½æˆåŠŸï¼")
        return tokenizer, model, image_processor, context_len
        
    except Exception as e:
        print(f"âŒ åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None

def test_model_inference(tokenizer, model, image_processor):
    print("\n=== æµ‹è¯•æ¨¡å‹æ¨ç† ===")
    
    try:
        # æµ‹è¯•æ–‡æœ¬
        test_prompt = "Does the image entails this statement: \"Test statement\"?"
        
        # åˆ›å»ºæµ‹è¯•å›¾åƒ
        from PIL import Image
        import numpy as np
        test_image = Image.fromarray(np.random.randint(0, 255, (224, 224, 3), dtype=np.uint8))
        
        # å¤„ç†è¾“å…¥
        from tinychart.conversation import conv_templates
        conv = conv_templates["phi"].copy()
        conv.append_message(conv.roles[0], test_prompt)
        conv.append_message(conv.roles[1], None)
        text = conv.get_prompt()
        
        inputs = tokenizer([text], return_tensors="pt").to("cuda")
        # å…³é”®ä¿®å¤ï¼šç¡®ä¿å›¾åƒå¼ é‡æ•°æ®ç±»å‹ä¸€è‡´
        image_tensor = image_processor.preprocess(test_image, return_tensors="pt")["pixel_values"][0].unsqueeze(0)
        image_tensor = image_tensor.to("cuda").to(model.dtype)
        
        # æ¨ç†
        with torch.no_grad():
            outputs = model(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                images=image_tensor,
                return_dict=True,
            )
        
        print(f"âœ… æ¨ç†æµ‹è¯•æˆåŠŸ! è¾“å‡ºå½¢çŠ¶: {outputs.logits.shape}")
        return True
        
    except Exception as e:
        print(f"âŒ æ¨ç†æµ‹è¯•å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # åŠ è½½æ¨¡å‹
    tokenizer, model, image_processor, context_len = load_lora_model()
    
    if model is not None:
        # æµ‹è¯•æ¨ç†
        success = test_model_inference(tokenizer, model, image_processor)
        
        if success:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å¯ä»¥ç”¨äºè¯„ä¼°ã€‚")
        else:
            print("\nâš ï¸ æ¨¡å‹åŠ è½½æˆåŠŸä½†æ¨ç†æµ‹è¯•å¤±è´¥ã€‚")
    else:
        print("\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥ã€‚")