import os
import sys

# ===== è®¾å®šåªä½¿ç”¨ GPU 4 =====

# ===== æ·»åŠ  mPLUG-Owl æºç è·¯å¾„ =====
sys.path.append("/data/jguo376/project/model/mPLUG-Owl/mPLUG-Owl")

# ===== å¯¼å…¥ä¾èµ– =====
import torch
from PIL import Image
from peft import LoraConfig, get_peft_model
from transformers import AutoTokenizer
from mplug_owl.modeling_mplug_owl import MplugOwlForConditionalGeneration
from mplug_owl.processing_mplug_owl import MplugOwlImageProcessor, MplugOwlProcessor

# ===== è·¯å¾„é…ç½® =====
base_model_path = "/data/jguo376/pretrained_models/mplug-owl-llama-7b"
lora_path = "/data/jguo376/pretrained_models/MMCA/mmca_lora_weights.bin"
image_path = "/data/jguo376/project/dataset/chartsumm/chart_images/test_k_2.png"

# ===== åŠ è½½æ¨¡å‹ & LoRA æƒé‡ =====
print("ğŸ“¦ åŠ è½½æ¨¡å‹ä¸ LoRA æƒé‡...")
model = MplugOwlForConditionalGeneration.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
peft_config = LoraConfig(
    target_modules=r'.*language_model.*\.(q_proj|v_proj)',
    inference_mode=True,
    r=8,
    lora_alpha=32,
    lora_dropout=0.05
)
model = get_peft_model(model, peft_config)
lora_weights = torch.load(lora_path, map_location="cpu")
model.load_state_dict(lora_weights, strict=False)

# ===== åŠ è½½å¤„ç†å™¨ =====
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
image_processor = MplugOwlImageProcessor.from_pretrained(base_model_path)
processor = MplugOwlProcessor(image_processor, tokenizer)

# ===== æ„é€ å¯¹è¯ Promptï¼ˆæ”¯æŒå¤æ‚æ¨ç†ï¼‰=====
prompt = """The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: <image>
Human: Provide a short analytical description of the chart based on the data it shows.
AI:"""

# ===== è¯»å–å›¾åƒ & æ„é€ è¾“å…¥ =====
image = Image.open(image_path).convert("RGB")
inputs = processor(text=[prompt], images=[image], return_tensors="pt")
inputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}
inputs = {k: v.to(model.device) for k, v in inputs.items()}

# ===== ç”Ÿæˆè¾“å‡º =====
generate_kwargs = {
    'do_sample': True,
    'top_k': 5,
    'max_length': 512
}
print("ğŸ§  æ­£åœ¨ç”Ÿæˆå›¾è¡¨æè¿°...")
with torch.no_grad():
    output = model.generate(**inputs, **generate_kwargs)
    caption = tokenizer.decode(output[0], skip_special_tokens=True)

# ===== è¾“å‡ºç»“æœ =====
print("\nğŸ“Š å›¾è¡¨æè¿°ç»“æœï¼š")
print(caption)