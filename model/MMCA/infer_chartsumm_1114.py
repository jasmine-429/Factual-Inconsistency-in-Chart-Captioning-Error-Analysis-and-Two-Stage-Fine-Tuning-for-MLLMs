import os
import sys
import json
from PIL import Image
import torch
from tqdm import tqdm

# ======= é…ç½®è·¯å¾„ =======
BASE_MODEL = "/data/jguo376/pretrained_models/mplug-owl-llama-7b"
LORA_PATH = "/data/jguo376/pretrained_models/MMCA/mmca_lora_weights.bin"
IMAGE_ROOT = "/data/jguo376/project/dataset/chartsumm/chart_images"
INPUT_JSON = "/data/jguo376/project/dataset/chartsumm/test_s.json"  # ä½ çš„è¾“å…¥æ–‡ä»¶
OUTPUT_JSONL = "/data/jguo376/project/model/mmca_caption/captions_mmca_s.jsonl"    # è¾“å‡ºæ–‡ä»¶

# ======= æ–­ç‚¹ç»­è·‘ï¼šåŠ è½½å·²å®Œæˆè®°å½• =======
done_images = set()
if os.path.exists(OUTPUT_JSONL):
    with open(OUTPUT_JSONL, "r") as f:
        for line in f:
            try:
                item = json.loads(line)
                done_images.add(item["image"])
            except:
                continue

print(f"ğŸ” å·²å®Œæˆ {len(done_images)} æ¡ï¼Œå°†è·³è¿‡è¿™äº›æ ·æœ¬")

# ======= æ·»åŠ  mPLUG-Owl æºç è·¯å¾„ =======
sys.path.append("/data/jguo376/project/model/mPLUG-Owl/mPLUG-Owl")

# ======= æ¨¡å‹åŠ è½½ =======
print("ğŸ“¦ åŠ è½½æ¨¡å‹...")
from peft import LoraConfig, get_peft_model
from transformers import AutoTokenizer
from mplug_owl.modeling_mplug_owl import MplugOwlForConditionalGeneration
from mplug_owl.processing_mplug_owl import MplugOwlImageProcessor, MplugOwlProcessor

model = MplugOwlForConditionalGeneration.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)

peft_cfg = LoraConfig(
    target_modules=r'.*language_model.*\.(q_proj|v_proj)',
    inference_mode=True,
    r=8,
    lora_alpha=32,
    lora_dropout=0.05
)
model = get_peft_model(model, peft_cfg)

print("ğŸ“¥ åŠ è½½ LoRA æƒé‡...")
lora_weights = torch.load(LORA_PATH, map_location="cpu")
model.load_state_dict(lora_weights, strict=False)

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)
image_processor = MplugOwlImageProcessor.from_pretrained(BASE_MODEL)
processor = MplugOwlProcessor(image_processor, tokenizer)

# ======= ç”Ÿæˆ Prompt =======
PROMPT = """The following is a conversation between a curious human and AI assistant. 
The assistant gives helpful, detailed, and polite answers.
Human: <image>
Human: Provide a short analytical description of the chart based on the data it shows.
AI:"""

# ======= è¯»å–æ•°æ® =======
with open(INPUT_JSON, "r") as f:
    dataset = json.load(f)

# ======= æ¨ç†å‚æ•° =======
GEN_KWARGS = {
    "do_sample": True,
    "top_k": 5,
    "max_length": 512,
}

# ======= å¼€å§‹æ¨ç† =======
print("ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆ caption...")

with open(OUTPUT_JSONL, "a") as fout:
    for item in tqdm(dataset):
        image_name = item["image"]

        # æ–­ç‚¹ç»­è·‘
        if image_name in done_images:
            continue

        image_path = os.path.join(IMAGE_ROOT, image_name)
        if not os.path.exists(image_path):
            print(f"âŒ æ‰¾ä¸åˆ°å›¾ç‰‡: {image_path}")
            continue

        # è¯»å–å›¾ç‰‡
        image = Image.open(image_path).convert("RGB")

        # æ„é€ è¾“å…¥
        inputs = processor(text=[PROMPT], images=[image], return_tensors="pt")
        inputs = {k: (v.bfloat16() if v.dtype == torch.float else v).to(model.device) for k, v in inputs.items()}

        # ç”Ÿæˆ
        with torch.no_grad():
            output = model.generate(**inputs, **GEN_KWARGS)
            caption = tokenizer.decode(output[0], skip_special_tokens=True)

        # å†™å‡º
        result = {"image": image_name, "caption": caption}
        fout.write(json.dumps(result, ensure_ascii=False) + "\n")

        # å®æ—¶ flushï¼Œä¾¿äºæ–­ç‚¹ç»­è·‘
        fout.flush()

print("ğŸ‰ å…¨éƒ¨å®Œæˆï¼")
