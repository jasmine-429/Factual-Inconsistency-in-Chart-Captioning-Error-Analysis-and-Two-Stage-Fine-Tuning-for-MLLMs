# merge_two_adapters_into_one.py
import os
import json
import shutil
import argparse
import torch

def load_adapter_bin(path):
    bin_path = os.path.join(path, "adapter_model.bin")
    if not os.path.exists(bin_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {bin_path}")
    return torch.load(bin_path, map_location="cpu")

def load_adapter_config(path):
    cfg_path = os.path.join(path, "adapter_config.json")
    if not os.path.exists(cfg_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ° {cfg_path}")
    with open(cfg_path, "r", encoding="utf-8") as f:
        return json.load(f)

def check_compat(cfg1, cfg2, keys=("peft_type","task_type","r","lora_alpha","lora_dropout","target_modules")):
    for k in keys:
        if cfg1.get(k) != cfg2.get(k):
            raise ValueError(f"LoRA é…ç½®ä¸ä¸€è‡´: key={k}, v1={cfg1.get(k)}, v2={cfg2.get(k)}")

def merge_state_dicts(sd1, sd2, w1=1.0, w2=1.0):
    merged = {}
    # ä»¥ sd1 çš„é”®ä¸ºåŸºå‡†ï¼Œè‹¥ sd2 æœ‰åŒåé”®åˆ™åˆå¹¶ï¼Œå¦åˆ™ä¿ç•™ sd1
    keys = set(sd1.keys()) | set(sd2.keys())
    for k in keys:
        v1 = sd1.get(k, None)
        v2 = sd2.get(k, None)
        if isinstance(v1, torch.Tensor) and isinstance(v2, torch.Tensor):
            # ç®€å•çº¿æ€§åˆå¹¶ï¼šv = w1*v1 + w2*v2
            merged[k] = w1 * v1 + w2 * v2
        elif isinstance(v1, torch.Tensor) and v2 is None:
            merged[k] = v1.clone()
        elif v1 is None and isinstance(v2, torch.Tensor):
            merged[k] = v2.clone()
        else:
            # æ—¢ä¸æ˜¯ Tensor å°±è·³è¿‡ï¼ˆå¸¸è§æ˜¯å…ƒæ•°æ®ï¼Œä¸å½±å“ï¼‰
            pass
    return merged

def main(args):
    lora1_dir = args.lora1
    lora2_dir = args.lora2
    out_dir   = args.out
    w1        = args.w1
    w2        = args.w2
    base_name = args.base  # å¯é€‰ï¼Œå†™å…¥ adapter_config.json é‡Œ

    os.makedirs(out_dir, exist_ok=True)

    print("ğŸ“¥ è¯»å– LoRA 1/2 çš„æƒé‡ä¸é…ç½®...")
    sd1 = load_adapter_bin(lora1_dir)
    sd2 = load_adapter_bin(lora2_dir)
    cfg1 = load_adapter_config(lora1_dir)
    cfg2 = load_adapter_config(lora2_dir)

    print("ğŸ” æ ¡éªŒå…³é”® LoRA é…ç½®æ˜¯å¦ä¸€è‡´ï¼ˆr/alpha/target_modules ç­‰ï¼‰...")
    check_compat(cfg1, cfg2)

    print(f"ğŸ§® åˆå¹¶æƒé‡ï¼šmerged = {w1} * lora1 + {w2} * lora2")
    merged_sd = merge_state_dicts(sd1, sd2, w1=w1, w2=w2)

    # å†™å‡ºåˆå¹¶åçš„ adapter_model.bin
    out_bin = os.path.join(out_dir, "adapter_model.bin")
    torch.save(merged_sd, out_bin)
    print(f"ğŸ’¾ å·²ä¿å­˜: {out_bin}")

    # ç”Ÿæˆåˆå¹¶åçš„ adapter_config.jsonï¼ˆä»¥ cfg1 ä¸ºåŸºå‡†ï¼Œå¯é€‰å†™å…¥ base_model_name_or_pathï¼‰
    merged_cfg = cfg1.copy()
    if args.base:
        merged_cfg["base_model_name_or_path"] = base_name
    out_cfg = os.path.join(out_dir, "adapter_config.json")
    with open(out_cfg, "w", encoding="utf-8") as f:
        json.dump(merged_cfg, f, indent=2, ensure_ascii=False)
    print(f"ğŸ’¾ å·²ä¿å­˜: {out_cfg}")

    print("âœ… å®Œæˆï¼ç°åœ¨ä½ å¯ä»¥åƒæ™®é€š LoRA ä¸€æ ·åŠ è½½è¿™ä¸ªåˆå¹¶åçš„é€‚é…å™¨ã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--lora1", required=True, help="LoRA 1 ç›®å½•ï¼ˆå« adapter_model.bin / adapter_config.jsonï¼‰")
    parser.add_argument("--lora2", required=True, help="LoRA 2 ç›®å½•")
    parser.add_argument("--out",   required=True, help="è¾“å‡ºç›®å½•ï¼ˆå°†ç”Ÿæˆåˆå¹¶åçš„ LoRAï¼‰")
    parser.add_argument("--w1",    type=float, default=1.0, help="LoRA1 æƒé‡ç³»æ•°")
    parser.add_argument("--w2",    type=float, default=1.0, help="LoRA2 æƒé‡ç³»æ•°")
    parser.add_argument("--base",  type=str, default="",     help="å¯é€‰ï¼šå†™å…¥ base_model_name_or_path å­—æ®µ")
    args = parser.parse_args()
    main(args)
