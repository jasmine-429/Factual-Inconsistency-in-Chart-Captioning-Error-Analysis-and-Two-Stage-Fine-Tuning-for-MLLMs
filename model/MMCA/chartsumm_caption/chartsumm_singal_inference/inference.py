import os
import sys

# ===== è®¾å®šç¯å¢ƒ =====

sys.path.append("/data/jguo376/project/model/mPLUG-Owl/mPLUG-Owl")

import json
from PIL import Image
from tqdm import tqdm
import torch
from peft import LoraConfig, get_peft_model
from transformers import AutoTokenizer
from mplug_owl.modeling_mplug_owl import MplugOwlForConditionalGeneration
from mplug_owl.processing_mplug_owl import MplugOwlImageProcessor, MplugOwlProcessor

# ===== è·¯å¾„é…ç½® =====
base_model_path = "/data/jguo376/pretrained_models/mplug-owl-llama-7b"
lora_path = "/data/jguo376/pretrained_models/MMCA/mmca_lora_weights.bin"
input_jsonl = "/data/jguo376/project/dataset/chartsumm/test_s.json"
output_json = "/data/jguo376/project/model/MMCA/chartsumm_caption/chartsumm_singal_inference/test_s_outpu.json"
chart_root = "/data/jguo376/project/dataset/chartsumm/chart_images/"


# ===== åŠ è½½æ¨¡å‹ä¸ LoRA æƒé‡ =====
print("ğŸ“¦ åŠ è½½æ¨¡å‹ä¸ LoRA æƒé‡...")
model = MplugOwlForConditionalGeneration.from_pretrained(
    base_model_path,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
peft_config = LoraConfig(
    target_modules=r'.*language_model.*\.(q_proj|v_proj)',
    inference_mode=True,
    r=8,
    lora_alpha=32,
    lora_dropout=0.05
)
model = get_peft_model(model, peft_config)
lora_weights = torch.load(lora_path, map_location="cpu", weights_only=True)

model.load_state_dict(lora_weights, strict=False)

# ===== åŠ è½½å¤„ç†å™¨ =====
tokenizer = AutoTokenizer.from_pretrained(base_model_path)
image_processor = MplugOwlImageProcessor.from_pretrained(base_model_path)
processor = MplugOwlProcessor(image_processor, tokenizer)

# ===== æ¨ç†å‚æ•° =====
generate_kwargs = {
    'do_sample': True,
    'top_k': 5,
    'max_length': 256
}
query_prompt = """The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: <image>
Human: Provide a short analytical description of the chart based on the data it shows.
AI:"""

# ===== åŠ è½½è¾“å…¥æ•°æ® =====
with open(input_jsonl, "r", encoding="utf-8") as f:
    data_list = json.load(f)

results = []
max_test = None  # è®¾ç½®ä¸ºæ•°å­—é™åˆ¶æ¡æ•°ï¼Œå¦‚ 5ï¼›None è¡¨ç¤ºå…¨éƒ¨å¤„ç†

# ===== æ‰¹é‡å¤„ç†å›¾è¡¨å›¾åƒ =====
print("ğŸš€ å¼€å§‹æ‰¹é‡ç”Ÿæˆå›¾è¡¨æè¿°...")
count = 0
for item in tqdm(data_list, desc="Generating captions"):
    if max_test is not None and count >= max_test:
        break


    imgname = item.get("image") or item.get("img")  # ä¸¤ç§å¯èƒ½å­—æ®µå
    image_path = os.path.join(chart_root, imgname)

    if not os.path.exists(image_path):
        caption = f"[ERROR] Image not found: {image_path}"
    else:
        try:
            image = Image.open(image_path).convert("RGB")
            inputs = processor(text=[query_prompt], images=[image], return_tensors="pt")
            inputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}
            inputs = {k: v.to(model.device) for k, v in inputs.items()}

            with torch.no_grad():
                output = model.generate(**inputs, **generate_kwargs)
                caption = tokenizer.decode(output[0], skip_special_tokens=True)
        except Exception as e:
            caption = f"[ERROR] {str(e)}"

    item["model_name"] = "mPLUG-Owl + MMCA"
    item["img"] = image_path  # ä½¿ç”¨ç»å¯¹è·¯å¾„
    item["generated_caption"] = caption
    results.append(item)
    count += 1

    print(f"[âœ“] {imgname}")
    print(f"    â†’ {caption}\n")

# ===== ä¿å­˜è¾“å‡ºæ–‡ä»¶ =====
with open(output_json, "w", encoding="utf-8") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

print(f"\nâœ… æ¨ç†å®Œæˆï¼Œå…±ç”Ÿæˆ {len(results)} æ¡å›¾è¡¨æè¿°ï¼Œè¾“å‡ºä¿å­˜è‡³ï¼š{output_json}")
