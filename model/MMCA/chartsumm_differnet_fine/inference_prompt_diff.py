import os
import sys
import json
import torch
from PIL import Image
from tqdm import tqdm
import traceback

# ========= ç¯å¢ƒè®¾ç½® =========
sys.path.append("/data/jguo376/project/model/mPLUG-Owl/mPLUG-Owl")
torch.set_grad_enabled(False)

from transformers import AutoTokenizer
from mplug_owl.modeling_mplug_owl import MplugOwlForConditionalGeneration
from mplug_owl.processing_mplug_owl import MplugOwlProcessor, MplugOwlImageProcessor
from peft import PeftModel

# ========= è·¯å¾„é…ç½® =========
model_path_for_weights = "/data/jguo376/pretrained_models/mmca_merged_model"
processor_ref_path = "/data/jguo376/pretrained_models/mplug-owl-llama-7b"
lora_path = "/data/jguo376/project/model/MMCA/fine-tuning/output/sft_v0.1_ft_chartsumm_caption_prompt/checkpoint-1500"

input_jsons = [
    "/data/jguo376/project/dataset/chartsumm/test_k.json",
    "/data/jguo376/project/dataset/chartsumm/test_s.json"
]
output_jsons = [
    "/data/jguo376/project/model/MMCA/chartsumm_differnet_fine/1500_new/test_k_output.json",
    "/data/jguo376/project/model/MMCA/chartsumm_differnet_fine/1500_new/test_s_output.json"
]
chart_root = "/data/jguo376/project/dataset/chartsumm/chart_images/"
max_items = None  # None è¡¨ç¤ºå…¨éƒ¨å¤„ç†

# ========= åŠ è½½æ¨¡å‹ =========
print("ğŸ“¦ åŠ è½½åŸºç¡€æ¨¡å‹æƒé‡...")
model = MplugOwlForConditionalGeneration.from_pretrained(
    model_path_for_weights,
    torch_dtype=torch.bfloat16,
    device_map="auto"
)
if lora_path:
    print(f"ğŸª„ åŠ è½½ LoRA adapter: {lora_path}")
    model = PeftModel.from_pretrained(model, lora_path)
    model.eval()
    # ğŸ”¥ å…³é”®éªŒè¯æ­¥éª¤
    print("ğŸ” éªŒè¯LoRAæ˜¯å¦ç”Ÿæ•ˆ...")
    
    # æ£€æŸ¥LoRAæ¨¡å—
    lora_modules = [name for name, module in model.named_modules() 
                   if hasattr(module, 'lora_A')]
    
    if lora_modules:
        print(f"âœ… æ‰¾åˆ° {len(lora_modules)} ä¸ªLoRAæ¨¡å—")
        print(f"   ç¤ºä¾‹æ¨¡å—: {lora_modules[0]}")
        
        # ğŸ”¥ ä¿®å¤åçš„æƒé‡æ•°å€¼æ£€æŸ¥
        print("ğŸ” æ£€æŸ¥LoRAæƒé‡æ•°å€¼...")
        total_lora_norm = 0
        zero_count = 0
        checked_count = 0
        
        for name, module in model.named_modules():
            if hasattr(module, 'lora_A') and hasattr(module, 'lora_B'):
                try:
                    # å¤„ç†ä¸åŒçš„LoRAç»“æ„
                    if hasattr(module.lora_A, 'weight'):
                        lora_A_norm = module.lora_A.weight.norm().item()
                    elif hasattr(module.lora_A, 'default'):
                        lora_A_norm = module.lora_A.default.weight.norm().item()
                    else:
                        # å¦‚æœæ˜¯ModuleDictï¼Œå°è¯•è·å–default key
                        if isinstance(module.lora_A, torch.nn.ModuleDict):
                            if 'default' in module.lora_A:
                                lora_A_norm = module.lora_A['default'].weight.norm().item()
                            else:
                                # è·å–ç¬¬ä¸€ä¸ªkey
                                first_key = list(module.lora_A.keys())[0]
                                lora_A_norm = module.lora_A[first_key].weight.norm().item()
                        else:
                            continue
                    
                    if hasattr(module.lora_B, 'weight'):
                        lora_B_norm = module.lora_B.weight.norm().item()
                    elif hasattr(module.lora_B, 'default'):
                        lora_B_norm = module.lora_B.default.weight.norm().item()
                    else:
                        if isinstance(module.lora_B, torch.nn.ModuleDict):
                            if 'default' in module.lora_B:
                                lora_B_norm = module.lora_B['default'].weight.norm().item()
                            else:
                                first_key = list(module.lora_B.keys())[0]
                                lora_B_norm = module.lora_B[first_key].weight.norm().item()
                        else:
                            continue
                    
                    total_lora_norm += lora_A_norm + lora_B_norm
                    
                    if lora_A_norm < 1e-6 and lora_B_norm < 1e-6:
                        zero_count += 1
                    
                    # åªæ‰“å°å‰5ä¸ªæ¨¡å—çš„è¯¦ç»†ä¿¡æ¯
                    if checked_count < 5:
                        print(f"  {name}:")
                        print(f"    lora_A norm: {lora_A_norm:.8f}")
                        print(f"    lora_B norm: {lora_B_norm:.8f}")
                        # æ‰“å°LoRAç»“æ„ä¿¡æ¯
                        print(f"    lora_A type: {type(module.lora_A)}")
                        print(f"    lora_B type: {type(module.lora_B)}")
                    
                    checked_count += 1
                    
                except Exception as e:
                    if checked_count < 3:  # åªæ‰“å°å‰å‡ ä¸ªé”™è¯¯
                        print(f"  âŒ æ£€æŸ¥ {name} æ—¶å‡ºé”™: {e}")
                        print(f"    lora_A type: {type(module.lora_A)}")
                        print(f"    lora_B type: {type(module.lora_B)}")
                        # å°è¯•æ‰“å°ç»“æ„
                        if isinstance(module.lora_A, torch.nn.ModuleDict):
                            print(f"    lora_A keys: {list(module.lora_A.keys())}")
                        if isinstance(module.lora_B, torch.nn.ModuleDict):
                            print(f"    lora_B keys: {list(module.lora_B.keys())}")
                    continue
        
        print(f"\nğŸ“Š LoRAæƒé‡ç»Ÿè®¡:")
        print(f"  æˆåŠŸæ£€æŸ¥çš„æ¨¡å—: {checked_count}/{len(lora_modules)}")
        if checked_count > 0:
            print(f"  æ€»æƒé‡èŒƒæ•°: {total_lora_norm:.8f}")
            print(f"  æ¥è¿‘é›¶çš„æ¨¡å—: {zero_count}/{checked_count}")
            print(f"  å¹³å‡æƒé‡èŒƒæ•°: {total_lora_norm/checked_count:.8f}")
            
            if total_lora_norm < 1e-3:
                print("âŒ ä¸¥é‡é—®é¢˜ï¼šæ‰€æœ‰LoRAæƒé‡éƒ½æ¥è¿‘é›¶ï¼è®­ç»ƒæ²¡æœ‰ç”Ÿæ•ˆï¼")
            elif zero_count > checked_count * 0.8:
                print("âš ï¸ è­¦å‘Šï¼šå¤§éƒ¨åˆ†LoRAæƒé‡æ¥è¿‘é›¶ï¼Œè®­ç»ƒå¯èƒ½ä¸å……åˆ†")
            else:
                print("âœ… LoRAæƒé‡çœ‹èµ·æ¥æ­£å¸¸")
        else:
            print("âŒ æ— æ³•æ£€æŸ¥ä»»ä½•LoRAæƒé‡")
            
        # ğŸ”¥ æ£€æŸ¥checkpointæ–‡ä»¶å¤§å°
        adapter_file = os.path.join(lora_path, "adapter_model.bin")
        if os.path.exists(adapter_file):
            file_size = os.path.getsize(adapter_file) / 1024 / 1024  # MB
            print(f"ğŸ“ adapter_model.bin å¤§å°: {file_size:.2f} MB")
            if file_size < 1.0:
                print("âš ï¸ è­¦å‘Šï¼šadapteræ–‡ä»¶å¾ˆå°ï¼Œå¯èƒ½æƒé‡æ›´æ–°ä¸å……åˆ†")
            else:
                print("âœ… adapteræ–‡ä»¶å¤§å°æ­£å¸¸")
        else:
            print(f"âŒ æœªæ‰¾åˆ° adapter_model.bin æ–‡ä»¶")
    else:
        print("âŒ è­¦å‘Šï¼šæœªæ‰¾åˆ°LoRAæ¨¡å—ï¼")
# ========= åŠ è½½ processor =========
tokenizer = AutoTokenizer.from_pretrained(processor_ref_path)
image_processor = MplugOwlImageProcessor.from_pretrained(processor_ref_path)
processor = MplugOwlProcessor(image_processor, tokenizer)

# ========= æ¨ç†å‚æ•° =========
generate_kwargs = {
    'do_sample': True,
    'top_k': 5,
    'max_length': 512
}

# ========= æ¨ç†å‡½æ•° =========
def run_inference(input_json, output_json, chart_root, query_prompt, max_items=None):
    print(f"\nğŸ“‚ å¤„ç†æ–‡ä»¶ï¼š{input_json}")
    
    if os.path.exists(output_json):
        with open(output_json, "r", encoding="utf-8") as f:
            try:
                existing_results = json.load(f)
            except json.JSONDecodeError:
                existing_results = []
        processed_imgs = set(item["image"] for item in existing_results)
    else:
        existing_results = []
        processed_imgs = set()

    results = existing_results.copy()

    with open(input_json, "r", encoding="utf-8") as f:
        data_list = json.load(f)

    if max_items is not None:
        data_list = data_list[:max_items]

    for idx, item in enumerate(tqdm(data_list, desc=f"Generating captions for {os.path.basename(input_json)}")):
        image_name = item.get("image") or item.get("img")
        img_path = os.path.join(chart_root, image_name)

        if image_name in processed_imgs:
            continue
        if not os.path.exists(img_path):
            print(f"âš ï¸ å›¾åƒä¸å­˜åœ¨: {img_path}")
            continue

        try:
            image = Image.open(img_path).convert("RGB")
            inputs = processor(text=[query_prompt], images=[image], return_tensors="pt")
            inputs = {k: v.bfloat16() if v.dtype == torch.float else v for k, v in inputs.items()}
            inputs = {k: v.to(model.device) for k, v in inputs.items()}

            with torch.no_grad():
                output = model.generate(**inputs, **generate_kwargs)
                caption = tokenizer.decode(output[0], skip_special_tokens=True)
        except Exception as e:
            print(f"âŒ é”™è¯¯ï¼š{image_name} - {e}")
            traceback.print_exc()
            caption = f"[ERROR] {str(e)}"

        result = {
            "image": image_name,
            "generated_caption": caption
        }
        results.append(result)
        processed_imgs.add(image_name)

        print(f"[âœ“] {image_name}")
        print(f"    â†’ {caption}\n")

        with open(output_json, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"âœ… å®Œæˆï¼š{output_json}ï¼Œå…±ç”Ÿæˆ {len(results)} æ¡æè¿°")


# ========= åˆ†åˆ«è®¾ç½® prompt å¹¶å¤„ç† =========
query_prompts = [
    """The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: <image>
Human: Please generate a short summary of the chart.
AI:""",
    """The following is a conversation between a curious human and AI assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.
Human: <image>
Human: Please generate a long summary of the chart.
AI:"""
]

for in_path, out_path, prompt in zip(input_jsons, output_jsons, query_prompts):
    run_inference(in_path, out_path, chart_root, prompt, max_items=max_items)
