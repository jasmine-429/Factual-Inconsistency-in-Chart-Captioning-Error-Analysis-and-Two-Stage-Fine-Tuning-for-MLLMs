import os
os.environ["CUDA_VISIBLE_DEVICES"] = "4"

import torch
from transformers import AutoTokenizer, AutoModel, AutoConfig
from PIL import Image

# å…³é—­æ¢¯åº¦ä»¥èŠ‚çœæ˜¾å­˜
torch.set_grad_enabled(False)

# æ¨¡å‹è·¯å¾„ï¼ˆHuggingFaceï¼‰
ckpt_path = "internlm/internlm-xcomposer2d5-7b"

# åŠ è½½é…ç½®å¹¶å¼ºåˆ¶ä½¿ç”¨æ™®é€š attention
config = AutoConfig.from_pretrained(ckpt_path, trust_remote_code=True)
config.attn_implementation = "eager"  # ğŸš« ä¸ä½¿ç”¨ flash attention

# åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(ckpt_path, trust_remote_code=True)
model = AutoModel.from_pretrained(
    ckpt_path,
    config=config,  # âœ… ä½¿ç”¨ä¿®æ”¹åçš„ config
    torch_dtype=torch.float16,
    trust_remote_code=True
).cuda().eval()
model.tokenizer = tokenizer  # æ˜¾å¼ç»‘å®š tokenizer

# å›¾åƒè·¯å¾„å’Œæ–‡æœ¬ prompt
query = "Write a concise paragraph that describes the chart, including key values, categories, and noticeable trends."
def ensure_rgb(image_path):
    img = Image.open(image_path)
    if img.mode != "RGB":
        img = img.convert("RGB")
        img.save(image_path)  # æˆ–å¦å­˜ä¸ºæ–°çš„è·¯å¾„
    return image_path

image_path = ensure_rgb("/data/jguo376/project/dataset/ChartX_dataset/ChartX/bar_chart/png/bar_5.png")
image = [image_path]

# æ¨ç†
with torch.autocast(device_type='cuda', dtype=torch.float16):
    response, _ = model.chat(
        tokenizer,
        query,
        image,
        do_sample=False,
        num_beams=3,
        use_meta=True
    )

print("ğŸ“Š å›¾è¡¨æè¿°ç»“æœï¼š", response)
