import os
os.environ["CUDA_VISIBLE_DEVICES"] = "4"
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

# è®¾ç½®æ¨¡å‹åç§°å’Œå›¾åƒè·¯å¾„
model_id = "Qwen/Qwen-VL-Chat"
image_path = "/data/jguo376/project/dataset/ChartX_dataset/ChartX/bar_chart/png/bar_34.png"

# åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)

model = AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="cuda",  # æˆ– "cpu"
    trust_remote_code=True,
    bf16=True,
    ignore_mismatched_sizes=True
).eval()

# æ„é€ æ¨ç†è¾“å…¥
query = tokenizer.from_list_format([
    {"image": image_path},
    {"text": "Provide a short analytical description of the chart based on the data it shows."}
])

# æ‰§è¡Œæ¨ç†
response, _ = model.chat(tokenizer, query=query, history=None)

# è¾“å‡ºç»“æœ
print("ğŸ“ å›¾è¡¨æè¿°ç»“æœï¼š")
print(response)
