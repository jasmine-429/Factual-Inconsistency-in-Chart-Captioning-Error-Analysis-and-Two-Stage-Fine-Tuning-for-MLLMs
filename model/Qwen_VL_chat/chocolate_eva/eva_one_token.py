import torch
from PIL import Image
from transformers import AutoProcessor, AutoModelForVision2Seq
import os

# ==== é…ç½®è·¯å¾„ ====
model_path = "/data/jguo376/pretrained_models/Qwen2.5-VL-7B-Instruct"
image_path = "/data/jguo376/project/dataset/ChartX_dataset/ChartX/bar_chart/png/bar_123.png"
prompt_text = 'Does the image entail this statement: "The number of students in Science is higher than in Arts?"'

# ==== åŠ è½½æ¨¡å‹å’Œå¤„ç†å™¨ ====
processor = AutoProcessor.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(model_path, device_map="cuda", torch_dtype=torch.float16, trust_remote_code=True).eval()

# ==== æ„é€  message å’Œ prompt ====
image = Image.open(image_path).convert("RGB")
messages = [{
    "role": "user",
    "content": [
        {"type": "image", "image": image},
        {"type": "text", "text": prompt_text}
    ]
}]
text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

# ==== ç¼–ç è¾“å…¥ ====
inputs = processor(text=[text], images=[image], return_tensors="pt", padding=True).to("cuda")

# ==== æ‰“å°è¾“å…¥ tokenï¼ˆå« assistant å¼€å§‹ä½ç½®ï¼‰ ====
print("ğŸ§¾ Input Prompt:")
print(processor.tokenizer.decode(inputs['input_ids'][0], skip_special_tokens=False))

# ==== å‰å‘ä¼ æ’­è·å– logits[-1] ====
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits  # shape: [1, seq_len, vocab_size]
    next_token_logits = logits[0, -1]  # é¢„æµ‹ assistant ç¬¬ä¸€ä¸ªç”Ÿæˆ token çš„ä½ç½®

# ==== æ˜¾ç¤º top-10 tokenï¼ˆåŸºäº logitsï¼‰====
print("\nğŸ”® Top-10 Predicted Tokens by logits[-1]:")
top_k = torch.topk(next_token_logits, k=10)
for i in range(10):
    tok_id = top_k.indices[i].item()
    tok = processor.tokenizer.decode([tok_id])
    print(f"{i+1}. Token: '{tok}' (id={tok_id}) | logit = {top_k.values[i].item():.4f}")

# ==== çœŸå®ç”Ÿæˆ 1 ä¸ª token ====
with torch.no_grad():
    generated = model.generate(**inputs, max_new_tokens=1)
    gen_token_id = generated[0, -1].item()
    gen_token = processor.tokenizer.decode([gen_token_id])

pred_token_id = torch.argmax(next_token_logits).item()
pred_token = processor.tokenizer.decode([pred_token_id])

print(f"\nğŸ§ª generate(): '{gen_token}' | logits.argmax(): '{pred_token}'")
