import os
import json
import torch
from PIL import Image
from tqdm import tqdm
from transformers import Pix2StructProcessor, Pix2StructForConditionalGeneration

# ===== æ¨¡å‹é…ç½® =====
os.environ["CUDA_VISIBLE_DEVICES"] = "7"
model_id = "google/matcha-chart2text-statista"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("ğŸ“¦ Loading model and processor...")
processor = Pix2StructProcessor.from_pretrained(model_id)
model = Pix2StructForConditionalGeneration.from_pretrained(model_id).to(device).eval()
query = "Please describe the chart."

# ===== è·¯å¾„é…ç½®ï¼ˆchartsumm æ•°æ®ï¼‰=====
image_root = "/data/jguo376/project/dataset/chartsumm/chart_images"
input_jsons = [
    "/data/jguo376/project/dataset/chartsumm/test_k.json",
    "/data/jguo376/project/dataset/chartsumm/test_s.json"
]
output_jsons = [
    "/data/jguo376/project/model/matcha/test_k_output.json",
    "/data/jguo376/project/model/matcha/test_s_output.json"
]

# ===== æ§åˆ¶å‚æ•° =====
max_test = None   # è®¾ä¸ºæ•´æ•°ä»…å¤„ç†å‰Næ¡ï¼›Noneè¡¨ç¤ºå¤„ç†å…¨éƒ¨
save_every = 20   # æ¯å¤„ç†Næ¡ä¿å­˜ä¸€æ¬¡ï¼ˆå¢é‡ä¿å­˜ï¼‰

# ===== æ¨ç†å‡½æ•° =====
def generate_caption(image_path: str, query_text: str) -> str:
    image = Image.open(image_path).convert("RGB")
    inputs = processor(images=image, text=query_text, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=512)
    caption = processor.batch_decode(outputs, skip_special_tokens=True)[0]
    return caption.replace("\x0A", "").strip()

# ===== ä¸»æµç¨‹ï¼šé€ä¸ªæ–‡ä»¶å¤„ç† =====
for input_json, output_json in zip(input_jsons, output_jsons):
    print(f"\nğŸš€ Start caption generation for: {input_json}")

    # è¯»å–è¾“å…¥
    with open(input_json, "r", encoding="utf-8") as f:
        data_list = json.load(f)

    if max_test is not None:
        data_list = data_list[:max_test]

    # æ–­ç‚¹ç»­è·‘ï¼šè¯»å–å·²å®Œæˆç»“æœï¼ˆä»…ä¸¤å­—æ®µï¼‰
    processed_imgs = set()
    results = []
    if os.path.exists(output_json):
        with open(output_json, "r", encoding="utf-8") as f:
            try:
                results = json.load(f)
                for entry in results:
                    # å·²æœ‰è¾“å‡ºä¸­çš„ image å­—æ®µä½œä¸ºå”¯ä¸€é”®
                    processed_imgs.add(entry.get("image"))
                print(f"ğŸ” Loaded {len(results)} existing results (resume enabled)")
            except Exception:
                # è‹¥è€æ–‡ä»¶éæœŸæœ›ç»“æ„ï¼Œåˆ™ä»ç©ºå¼€å§‹
                results = []
                processed_imgs = set()

    new_buffer = []

    # æ¨ç†
    pbar = tqdm(data_list, desc=f"Generating captions for {os.path.basename(input_json)}")
    for item in pbar:
        image_name = item.get("image")
        if not image_name:
            # ç¼ºå°‘ image å­—æ®µåˆ™è·³è¿‡
            continue

        if image_name in processed_imgs:
            # å·²å¤„ç†è¿‡ï¼Œè·³è¿‡
            continue

        image_path = os.path.join(image_root, image_name)

        # ç”Ÿæˆ caption
        if not os.path.exists(image_path):
            caption = f"[ERROR] Image not found: {image_path}"
        else:
            try:
                caption = generate_caption(image_path, query)
            except Exception as e:
                caption = f"[ERROR] {str(e)}"

        # åªä¿ç•™ä¸¤å­—æ®µ
        out_rec = {
            "image": image_name,
            "generated_caption": caption
        }
        results.append(out_rec)
        new_buffer.append(out_rec)
        processed_imgs.add(image_name)

        # å¢é‡ä¿å­˜
        if len(new_buffer) >= save_every:
            with open(output_json, "w", encoding="utf-8") as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            new_buffer.clear()

    # æœ€ç»ˆä¿å­˜
    if new_buffer:
        with open(output_json, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"âœ… Done! Total: {len(results)} captions saved to: {output_json}")
