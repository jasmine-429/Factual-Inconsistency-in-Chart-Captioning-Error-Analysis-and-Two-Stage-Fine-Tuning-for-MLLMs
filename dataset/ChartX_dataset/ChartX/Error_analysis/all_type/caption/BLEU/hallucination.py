import os
import json
import re
import pycountry
import spacy
import pandas as pd
from tqdm import tqdm

nlp = spacy.load("en_core_web_sm")

def extract_years_from_caption(text):
    doc = nlp(text)
    return set(re.findall(r"\b(?:19|20)\d{2}\b", " ".join(ent.text for ent in doc.ents if ent.label_ == "DATE")))

def extract_years_regex(text):
    return set(re.findall(r"\b(?:19|20)\d{2}\b", text))

def extract_countries_from_csv(csv_text):
    countries = set()
    csv_text = csv_text.replace("\\t", "\t").replace("\\n", "\n")
    lines = csv_text.split("\n")
    for line in lines:
        cols = line.strip().split("\t")
        if not cols:
            continue
        first_col = cols[0].strip()
        if not first_col or first_col.lower() == "country":
            continue
        try:
            countries.add(pycountry.countries.lookup(first_col).name)
        except LookupError:
            alias_map = {
                "USA": "United States", "US": "United States", "U.S.": "United States",
                "UK": "United Kingdom", "U.K.": "United Kingdom", "Korea": "South Korea"
            }
            if first_col.upper() in alias_map:
                countries.add(alias_map[first_col.upper()])
    return countries

def extract_and_standardize_countries(text):
    country_set = set()
    doc = nlp(text)

    # âœ… åˆæ³•æ­£å¼å›½å®¶åé›†åˆï¼ˆç²¾ç¡®åŒ¹é…ï¼‰
    official_country_names = set(country.name for country in pycountry.countries)

    # âœ… åˆ«åæ˜ å°„ï¼ˆå¤§å†™keyç»Ÿä¸€åŒ¹é…ï¼‰
    alias_map = {
        "USA": "United States", "US": "United States", "U.S.": "United States",
        "UK": "United Kingdom", "U.K.": "United Kingdom", "KOREA": "South Korea"
    }

    # âœ… ç¦æ­¢è¯¯è¯†åˆ«çš„ä¸€äº›å›ºå®šçŸ­è¯­ï¼ˆä¾‹å¦‚å•ä½ï¼‰
    forbidden_phrases = [
        "U.S. dollars", "US dollars", "United States dollars", "U.S. Dollar", "US Dollar"
    ]
    lower_text = text.lower()  # å…¨éƒ¨å°å†™ä¾¿äºåŒ¹é…å®Œæ•´çŸ­è¯­

    # === 1. ç²¾ç¡®åŒ¹é… NER ä¸­çš„ GPE/LOC å®ä½“ ===
    for ent in doc.ents:
        if ent.label_ in ["GPE", "LOC"]:
            ent_text = ent.text.strip()
            ent_upper = ent_text.upper()
            ent_lower = ent_text.lower()

            if ent_text in official_country_names:
                country_set.add(ent_text)
            elif ent_upper in alias_map:
                # âœ… å¦‚æœå®ä½“ä½œä¸º forbidden çŸ­è¯­ä¸€éƒ¨åˆ†å­˜åœ¨åˆ™è·³è¿‡
                if any(ent_lower in phrase and phrase in lower_text for phrase in forbidden_phrases):
                    continue
                country_set.add(alias_map[ent_upper])

    # === 2. ä¸¥æ ¼æ­£åˆ™åŒ¹é…ç–‘ä¼¼å›½å®¶åï¼ˆé¿å… "and" â†’ "Andorra"ï¼‰===
    tokens = re.findall(r"\b[A-Z][a-z]{3,}(?:\s+[A-Z][a-z]+)*\b", text)
    for token in tokens:
        if token in official_country_names:
            country_set.add(token)

    # === 3. åŒ¹é…å¸¸è§ç¼©å†™åˆ«åï¼ˆæ’é™¤å•ä½ç±»çŸ­è¯­ï¼‰===
    for abbr in re.findall(r"\b[A-Z][A-Z\.]+\b", text):
        abbr_clean = abbr.replace(".", "").upper()
        if abbr_clean in alias_map:
            # âœ… å¦‚æœè¯¥ç¼©å†™åœ¨ forbidden phrase ä¸­ä½œä¸ºå‰ç¼€ï¼ˆå¦‚ U.S. in U.S. dollarsï¼‰ï¼Œåˆ™è·³è¿‡
            skip = False
            for phrase in forbidden_phrases:
                if phrase.lower().startswith(abbr.lower()) and phrase.lower() in lower_text:
                    skip = True
                    break
            if skip:
                continue
            country_set.add(alias_map[abbr_clean])

    return country_set


def analyze_one_model(json_path, preview_limit=5):
    with open(json_path, 'r') as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            f.seek(0)
            data = [json.loads(line) for line in f if line.strip()]

    model_name = data[0].get("model_name", os.path.splitext(os.path.basename(json_path))[0])
    total, year_hall, country_hall, overall = 0, 0, 0, 0
    preview_printed = 0

    print(f"\nğŸ“Š æ¨¡å‹: {model_name}")
    for item in tqdm(data, desc=model_name):
        caption = item.get("generated_caption", "")
        title = item.get("title", "")
        csv_text = item.get("csv", "")

        caption_years = extract_years_from_caption(caption)
        context_years = extract_years_regex(title) | extract_years_regex(csv_text)
        caption_countries = extract_and_standardize_countries(caption)
        context_countries = extract_and_standardize_countries(title) | extract_countries_from_csv(csv_text)

        year_hallucination = any(y not in context_years for y in caption_years)
        country_hallucination = any(c not in context_countries for c in caption_countries)

        if year_hallucination: year_hall += 1
        if country_hallucination: country_hall += 1
        if year_hallucination or country_hallucination: overall += 1
        total += 1

        if preview_printed < preview_limit:
            print(f"\nâ€”â€” Sample {preview_printed+1} â€”â€”")
            print(f"ğŸ“· å›¾åƒå: {item.get('imgname', '')}")
            print(f"ğŸ•“ Caption ä¸­å¹´ä»½: {list(caption_years)}  |  Title/CSV ä¸­å¹´ä»½: {list(context_years)}")
            print(f"ğŸ§  æ—¶é—´å¹»è§‰: {year_hallucination}")
            print(f"ğŸŒ Caption ä¸­å›½å®¶: {list(caption_countries)}  |  Title/CSV ä¸­å›½å®¶: {list(context_countries)}")
            print(f"ğŸ§  åœ°ç‚¹å¹»è§‰: {country_hallucination}")
            preview_printed += 1

    return {
        "model_name": model_name,
        "total": total,
        "year_hallucination": year_hall,
        "country_hallucination": country_hall,
        "overall_hallucination": overall
    }

def batch_process_model_files(folder_path, output_csv_path, preview_limit=5):
    results = []
    for filename in sorted(os.listdir(folder_path)):
        if filename.endswith(".json"):
            json_path = os.path.join(folder_path, filename)
            try:
                result = analyze_one_model(json_path, preview_limit=preview_limit)
                results.append(result)
            except Exception as e:
                print(f"âŒ è·³è¿‡ {filename}ï¼Œé”™è¯¯: {e}")
    pd.DataFrame(results).to_csv(output_csv_path, index=False)
    print(f"\nâœ… æ‰€æœ‰æ¨¡å‹ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°: {output_csv_path}")

if __name__ == "__main__":
    input_folder = "/data/jguo376/project/dataset/ChartX_dataset/ChartX/Error_analysis/all_type/caption/deep_analysis/caption_all"
    output_csv = "/data/jguo376/project/dataset/ChartX_dataset/ChartX/Error_analysis/all_type/caption/BLEU/hallucination_stats_by_model.csv"
    batch_process_model_files(input_folder, output_csv, preview_limit=5)
