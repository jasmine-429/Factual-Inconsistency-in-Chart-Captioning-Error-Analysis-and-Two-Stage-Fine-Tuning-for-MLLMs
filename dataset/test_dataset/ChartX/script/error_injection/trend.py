import json
import re
from tqdm import tqdm
from collections import OrderedDict

# ===== è·¯å¾„é…ç½® =====
input_path = "/data/jguo376/project/dataset/test_dataset/ChartX/test_data/dataset/test_samples_id.json"
output_path = "/data/jguo376/project/dataset/test_dataset/ChartX/test_data/dataset/error_data/train_trend_errors.json"
log_path = "/data/jguo376/project/dataset/test_dataset/ChartX/test_data/dataset/error_data/error_log/trend_magnitude_error_doc.txt"

# ===== æ„é€ åä¹‰è¯è¯å…¸ï¼ˆå¯æ‰©å±•ï¼‰=====
ANTONYM_DICT = {
    # trend
    "increase": "decrease", "increases": "decreases", "increased": "decreased", "increasing": "decreasing",
    "rise": "fall", "rises": "falls", "rose": "fell", "rising": "falling",
    "grow": "decline", "grew": "declined", "growing": "declining",

    "climb": "drop", "climbs": "drops", "climbed": "dropped", "climbing": "dropping",
    "soar": "plummet", "soars": "plummets", "soared": "plummeted", "soaring": "plummeting",

    # magnitude
    "sharp": "slight", "sharply": "slightly",
    "dramatic": "modest", "dramatically": "modestly",
    "marked": "negligible", "markedly": "negligibly",
    "abrupt": "slow", "abruptly": "slowly",
    "substantial": "minimal", "substantially": "minimally",
    "intense": "faint", "intensely": "faintly",
}

TREND_WORDS = {
    "increase", "increases", "increased", "increasing",
    "rise", "rises", "rose", "rising",
    "grow", "grew", "growing", "decline", "declined", "declining",
    "climb", "climbs", "climbed", "climbing",
    "soar", "soars", "soared", "soaring",
}

MAGNITUDE_WORDS = {
    "sharp", "sharply", "dramatic", "dramatically",
    "marked", "markedly", "abrupt", "abruptly",
    "substantial", "substantially", "intense", "intensely",
}

# ===== åŠ è½½æ•°æ® =====
with open(input_path, "r") as f:
    data = json.load(f)

results = []
log_entries = []

for item in tqdm(data, desc="Injecting errors"):
    if item.get("label") != 1:
        continue

    sentence = item["sentence"]
    found_words = []

    for word, antonym in ANTONYM_DICT.items():
        pattern = r'\b' + re.escape(word) + r'\b'
        if re.search(pattern, sentence, flags=re.IGNORECASE):
            found_words.append((word, antonym))

    used_types = set()

    for word, antonym in found_words:
        if word in TREND_WORDS and "trend_error" in used_types:
            continue
        if word in MAGNITUDE_WORDS and "magnitude_error" in used_types:
            continue

        # æ›¿æ¢è¯ï¼Œç”Ÿæˆä¿®æ”¹å¥å­
        new_sentence = re.sub(r'\b' + re.escape(word) + r'\b', antonym, sentence, count=1, flags=re.IGNORECASE)
        error_type = "trend_error" if word in TREND_WORDS else "magnitude_error"
        used_types.add(error_type)

        # æ„å»ºæ–° entryï¼Œæ’å…¥ error å­—æ®µï¼Œlabel = 0
        new_item = OrderedDict()
        for k in item:
            new_item[k] = item[k]
            if k == "imgname":
                new_item["id"] = item["id"]
        new_item["error"] = error_type
        new_item["sentence"] = new_sentence
        new_item["label"] = 0

        results.append(new_item)
        log_entries.append(f"{item['id']} | {error_type} | {word} â†’ {antonym}")

# ===== ä¿å­˜æ³¨å…¥åçš„æ ·æœ¬ =====
with open(output_path, "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)

# ===== ä¿å­˜æ³¨å…¥æ—¥å¿— =====
with open(log_path, "w") as f:
    for entry in log_entries:
        f.write(entry + "\n")

print(f"âœ… é”™è¯¯æ³¨å…¥å®Œæˆï¼Œå…±ç”Ÿæˆ {len(results)} æ¡æ ·æœ¬ï¼Œä¿å­˜è‡³ï¼š{output_path}")
print(f"ğŸ“ æ³¨å…¥è¯è®°å½•å·²ä¿å­˜è‡³ï¼š{log_path}")
